{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ascii-code.com/ Charcter encoding standard used to represent text\n",
    "# How do we intialise our vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'Data Science and Data Engineering Friday sessions')\n"
     ]
    }
   ],
   "source": [
    "# Bits and Bytes\n",
    "text = \"Data Science and Data Engineering Friday sessions\"\n",
    "byte_arr = bytearray(text, 'utf-8') # byte array\n",
    "print(byte_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 97, 116, 97, 32, 83, 99, 105, 101, 110, 99, 101, 32, 97, 110, 100, 32, 68, 97, 116, 97, 32, 69, 110, 103, 105, 110, 101, 101, 114, 105, 110, 103, 32, 70, 114, 105, 100, 97, 121, 32, 115, 101, 115, 115, 105, 111, 110, 115]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_arr)\n",
    "print(ids) # for n number of characters, we have n number of tokenID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x7f\\x80\\x81\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x8b\\x8c\\x8d\\x8e\\x8f\\x90\\x91\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\x9b\\x9c\\x9d\\x9e\\x9f\\xa0\\xa1\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xab\\xac\\xad\\xae\\xaf\\xb0\\xb1\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xbb\\xbc\\xbd\\xbe\\xbf\\xc0\\xc1\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xcb\\xcc\\xcd\\xce\\xcf\\xd0\\xd1\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xdb\\xdc\\xdd\\xde\\xdf\\xe0\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xeb\\xec\\xed\\xee\\xef\\xf0\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xfb\\xfc\\xfd\\xfe\\xff')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASCII 256 possibles values -> 8 bits 2^8 = 256\n",
    "# 0 - 255\n",
    "# bytearray(range(0, 256))\n",
    "# problem with ourt initial vocabis missing some characters \"`\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 254]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "#for i in range(256):\n",
    "    # char = chr(i)\n",
    "    # token_id = gpt2_tokenizer.encode(char)[0]\n",
    "    # print(f'{char}: {token_id}')\n",
    "    # break\n",
    "\n",
    "# for i in range(256):\n",
    "#     token_id = gpt2_tokenizer.decode([i])\n",
    "#     print(f'{i}: {token_id}')\n",
    "\n",
    "\n",
    "# chr(33) -> '!'\n",
    "# chr(0) -> '\\x00'\\\n",
    "# In particular, the space character is 32, which we can see by ord(' '). Instead, this function will shift space (32) by 256 to 288, so d[32] -> 'Ġ'.\n",
    "# Bascially the=y need somethig that is visually, and eaily expr?\n",
    "# chr(32)\n",
    "# ord(' ')\n",
    "\n",
    "# gpt2_tokenizer.encode(\"Ġ\")\n",
    "# all the not nice cgarcater coke after 256\n",
    "# or b in range(2**8):\n",
    "        # if b not in bs:\n",
    "        #     bs.append(b)\n",
    "        #     cs.append(2**8+n)\n",
    "        #     n += 1\n",
    "\n",
    "# chr(256) Is going to be our space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of the BPE tokenization algorithm is to construct a vocabulary of frequently\n",
    "# occurring subwords, such as \"ing\"\n",
    "# (which appears in words like running, jumping, singing, bringing, thinking, ...), \n",
    "# or even entire words like \"apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
